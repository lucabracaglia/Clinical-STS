{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZD4BoDHErkOX"},"outputs":[],"source":["#!pip install sentence-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14062,"status":"ok","timestamp":1689256139216,"user":{"displayName":"Luca Bracaglia","userId":"02562360566548543549"},"user_tz":-120},"id":"udJeqJEpf4Jd","outputId":"6f696181-be96-41b0-f462-05b04ca3a72c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4868,"status":"ok","timestamp":1689256208747,"user":{"displayName":"Luca Bracaglia","userId":"02562360566548543549"},"user_tz":-120},"id":"lnF7_MY07VIn","outputId":"4bb5b024-c184-42ba-9be7-45812ee5be4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}],"source":["# Install packages\n","!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHXWjJ1Nqj6H"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import pearsonr\n","from google.colab import drive\n","import re\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"gTW8nyHNDbWm"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51201,"status":"ok","timestamp":1689256284289,"user":{"displayName":"Luca Bracaglia","userId":"02562360566548543549"},"user_tz":-120},"id":"XGRSaW3t6xXm","outputId":"bb4a7dcb-fa93-42a4-9395-8e536efa944d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"RkkSf-hKDUc0"},"source":["Viene inserito la tokenizzazione e poi lo stemming, provare anche con lemmatizzazione"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4489,"status":"ok","timestamp":1689260115522,"user":{"displayName":"Luca Bracaglia","userId":"02562360566548543549"},"user_tz":-120},"id":"a4kjQ6skq0wv","outputId":"a4170a8c-09d5-46c2-c999-f6c530749188"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at medicalai/ClinicalBERT were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","#from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from transformers import AutoTokenizer, AutoModel\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"medicalai/ClinicalBERT\")\n","model = AutoModel.from_pretrained(\"medicalai/ClinicalBERT\")\n","\n","nltk.download('stopwords')  # Download nltk stopwords\n","#nltk.download('punkt')      # Download nltk punkt tokenizer\n","#nltk.download('wordnet')\n","\n","text = []\n","stopwords_list = set(stopwords.words(\"english\"))\n","\n","#lemmatizer = WordNetLemmatizer()\n","\n","with open('/content/drive/MyDrive/deep_learning/progetto_deep/Semantic Textual Similarity/Clinical Semantic Textual Similarity/clinicalSTS2019/train/clinicalSTS2019.train.txt', 'r') as f:\n","    for line in f.readlines():\n","        line = line.strip()\n","\n","        # Splitting by tab\n","        line_parts = re.split('\\t', line)\n","\n","        filtered_sentences = []\n","        for sentence in line_parts:\n","            # Lowercasing\n","            lower_sentence = sentence.lower()\n","\n","            # Removing punctuation except periods\n","            processed_sentence = re.sub(r'[^\\w\\s.]', '', lower_sentence)\n","\n","            # Replace dots between numbers with commas\n","            processed_sentence = re.sub(r'(?<=\\d)\\.(?=\\d)', ',', processed_sentence)\n","\n","            # Remove remaining dots\n","            processed_sentence = processed_sentence.replace('.', '')\n","\n","            # Replace dots between numbers with dots\n","            processed_sentence = re.sub(r'(?<=\\d),(?=\\d)', '.', processed_sentence)\n","\n","            tokenized_sentence = tokenizer.tokenize(processed_sentence)\n","\n","            # Rimozione delle stopwords\n","            filtered_tokens = [token for token in tokenized_sentence if token.lower() not in stopwords_list]\n","\n","            filtered_sentences.append(filtered_tokens)\n","\n","        text.append(filtered_sentences)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1689261308828,"user":{"displayName":"Luca Bracaglia","userId":"02562360566548543549"},"user_tz":-120},"id":"OC2mUsLvMB4o"},"outputs":[],"source":["#text[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0Wq2JjRoJOC"},"outputs":[],"source":["def make_split(embedded_representation, labels):\n","    X_train, X_test, y_train, y_test = train_test_split(embedded_representation, labels, random_state=104, test_size=0.25, shuffle=True)\n","\n","    y_train = np.array(y_train, dtype=object)\n","    y_test = np.array(y_test, dtype=object)\n","\n","    X_train = np.array(X_train, dtype=object)\n","    X_test = np.array(X_test, dtype=object)\n","\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G70AGA6lou2O"},"outputs":[],"source":["def cosine_similarity_unidimensional(v1, v2):\n","    dot_product = np.dot(v1, v2)\n","    norm_v1 = np.linalg.norm(v1)\n","    norm_v2 = np.linalg.norm(v2)\n","    similarity = dot_product / (norm_v1 * norm_v2)\n","    return similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tT4ZeKYhpVpO"},"outputs":[],"source":["def normalize_train(y):\n","  y = [float(val) for val in y]  # Converti i valori in numeri decimali\n","  min_val = np.min(y)\n","  max_val = np.max(y)\n","  normalized_labels_train = (y - min_val) / (max_val - min_val) * 2 - 1\n","\n","  return normalized_labels_train\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xcaZXORgmv0B"},"outputs":[],"source":["# Definisci una lista di modelli da provare\n","models = [\n","    'bert-base-nli-mean-tokens',\n","    'bert-base-nli-max-tokens',\n","    'bert-base-nli-cls-token'\n","]\n","\n","all_results={}\n","\n","for model_name in models:\n","    # Carica il modello\n","    model = SentenceTransformer(model_name)\n","    text_embedded=[]\n","    label=[]\n","    train_cosine_similarity = []\n","\n","    for phrases in range(len(text)):\n","      sentence_vectors = model.encode(text[phrases][0:2])\n","      text_embedded.append(sentence_vectors)\n","      label.append((text[phrases][2]))\n","\n","    #Print the embeddings\n","    '''\n","    for sentence, embedding in zip(text, text_embedded):\n","      print(\"Sentence:\", sentence)\n","      print(\"Embedding:\", embedding)\n","      print()\n","    '''\n","\n","    X_train, X_test, y_train, y_test = make_split(text_embedded, label)\n","\n","    for sentence in X_train:\n","      sentence1 = sentence[0]\n","      sentence2 = sentence[1]\n","      similarity = cosine_similarity_unidimensional(sentence1, sentence2)\n","      train_cosine_similarity.append(similarity)\n","\n","    normalized_labels_train= normalize_train(y_train)\n","\n","    # Calcolo della correlazione di Pearson\n","    correlation, p_value = pearsonr(train_cosine_similarity, normalized_labels_train)\n","\n","    # Salva i risultati nel dizionario\n","    all_results[model_name] = {\n","        #'text_embedded': text_embedded,\n","        #'label': label,\n","        #'train_similarity': train_cosine_similarity,\n","        'correlation': correlation,\n","        'p_value': p_value\n","    }"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
